---
title: "Dog-SEP"
subtitle: "Machine learning: variable selection methods"
pagetitle: "Dog-SEP: variable selection"
author: "Radoslaw Panczak"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document: 
    highlight: pygments
    keep_md: no
    number_sections: yes
    theme: united
    toc: yes
    toc_depth: 4
    self_contained: true
    toc_float: yes
editor_options: 
  chunk_output_type: console
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "../docs") })
---

<!-- ------------------------------------------------------------ --> 

# Setup 

```{r r-setup, include=FALSE}
set.seed(12345)
options(scipen = 999)
options(max.print = "75")

library(pacman)
p_load(tidyverse, magrittr, DT,
       leaps, glmnet)

import::from("sjmisc", "frq")
```

```{r knit-setup, include=FALSE}
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file(),
                     width = 75)

knitr::opts_chunk$set(
  echo = TRUE,
  cache = FALSE,
  prompt = FALSE,
  tidy = FALSE,
  comment = NA,
  message = FALSE,
  warning = FALSE,
  fig.width = 8, fig.height = 6, dpi = 300,
  out.width = "800px", out.height = "600px"
)
```

<!-- ------------------------------------------------------------ --> 

## Background 

Many available adapted from ISLR ch 6.  

```{r include=FALSE}
training <- read_rds("data/training.Rds")
testing <- read_rds("data/testing.Rds")

data <- bind_rows(training, testing)
```

<!-- ------------------------------------------------------------ --> 

## Subset Selection Methods

### Best Subset Selection

```{r}
regfit.full <- regsubsets(outcome ~ ., data = data, vnvmax = 20)

summary(regfit.full)

reg.summary <- summary(regfit.full)

names(reg.summary)

reg.summary$rsq

par(mfrow = c(2, 2))
plot(reg.summary$rss, xlab = "Number of Variables",
     ylab = "RSS", type = "l")
plot(reg.summary$adjr2, xlab = "Number of Variables",
     ylab = "Adjusted RSq", type = "l")

which.max(reg.summary$adjr2)
points(11, reg.summary$adjr2[11], col = "red", cex = 2, 
       pch = 20)
plot(reg.summary$cp, xlab = "Number of Variables",
     ylab = "Cp", type = "l")

which.min(reg.summary$cp)
points(10, reg.summary$cp[10], col = "red", cex = 2,
       pch = 20)

which.min(reg.summary$bic)
plot(reg.summary$bic, xlab = "Number of Variables",
     ylab = "BIC", type = "l")
points(6, reg.summary$bic[6], col = "red", cex = 2,
       pch = 20)

plot(regfit.full, scale = "r2")
plot(regfit.full, scale = "adjr2")
plot(regfit.full, scale = "Cp")
plot(regfit.full, scale = "bic")

coef(regfit.full, 6)
```

### Forward and Backward Stepwise Selection

```{r}
regfit.fwd <- regsubsets(outcome ~ ., data = data, 
                         vnvmax = 20, method = "forward")
summary(regfit.fwd)

regfit.bwd <- regsubsets(outcome ~ ., data = data,
                         nvmax = 20, method = "backward")
summary(regfit.bwd)

coef(regfit.full, 7)
coef(regfit.fwd, 7)
coef(regfit.bwd, 7)
```

### Choosing Among Models Using the Validation-Set Approach and Cross-Validation

```{r}
regfit.best <- regsubsets(outcome ~ ., data = training,
                         nvmax = 20)

test.mat <- model.matrix(outcome ~ ., data = testing)

val.errors <- rep(NA, 20)
for (i in 1:20) {
  coefi <- coef(regfit.best, id = i)
  pred <- test.mat[, names(coefi)] %*% coefi
  val.errors[i] <- mean((testing$outcome - pred)^2)
}

val.errors
which.min(val.errors)
coef(regfit.best, which.min(val.errors))

predict.regsubsets <- function(object, newdata, id, ...) {
  form <- as.formula(object$call[[2]])
  mat <- model.matrix(form, newdata)
  coefi <- coef(object, id = id)
  xvars <- names(coefi)
  mat[, xvars] %*% coefi
}

regfit.best <- regsubsets(outcome ~ ., data = data, nvmax = 20)
coef(regfit.best, 7)

k <- 10
n <- nrow(Hitters)

folds <- sample(rep(1:k, length = n))
cv.errors <- matrix(NA, k, 20, dimnames = list(NULL, paste(1:20)))

for (j in 1:k) {
  best.fit <- regsubsets(outcome ~ .,
                         data = data[folds != j, ],
                         nvmax = 20)
  for (i in 1:20) {
    # predict below is actually predict.regsubsets
    pred <- predict(best.fit, data[folds == j, ], id = i)
    cv.errors[j, i] <-
      mean((data$outcome[folds == j] - pred)^2)
  }
}

dim(cv.errors)

# average over the columns of the matrix
mean.cv.errors <- apply(cv.errors, 2, mean)
mean.cv.errors

par(mfrow = c(1, 1))
plot(mean.cv.errors, type = "b")

reg.best <- regsubsets(outcome ~ ., data = data, nvmax = 20)
coef(reg.best, 10)
```

## Ridge Regression

```{r}
x <- model.matrix(outcome ~ ., data)[, -1]
y <- data$outcome

grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(x, y, alpha = 0, lambda = grid)

dim(coef(ridge.mod))

ridge.mod$lambda[50]
coef(ridge.mod)[, 50]
sqrt(sum(coef(ridge.mod)[-1, 50]^2))

ridge.mod$lambda[60]
coef(ridge.mod)[, 60]
sqrt(sum(coef(ridge.mod)[-1, 60]^2))

predict(ridge.mod, s = 50, type = "coefficients")[1:20, ]
```

```{r}
train <- sample(1:nrow(x), nrow(x) / 2)
test <- (-train)
y.test <- y[test]

ridge.mod <- glmnet(x[train, ], y[train], alpha = 0,
                    lambda = grid, thresh = 1e-12)

ridge.pred <- predict(ridge.mod, s = 4, newx = x[test, ])
mean((ridge.pred - y.test)^2)

mean((mean(y[train]) - y.test)^2)

ridge.pred <- predict(ridge.mod, s = 1e10, newx = x[test, ])
mean((ridge.pred - y.test)^2)

ridge.pred <- predict(ridge.mod, s = 0, newx = x[test, ],
                      exact = T, x = x[train, ], y = y[train])
mean((ridge.pred - y.test)^2)
lm(y ~ x, subset = train)
predict(ridge.mod, s = 0, exact = T, type = "coefficients",
        x = x[train, ], y = y[train])[1:20, ]


cv.out <- cv.glmnet(x[train, ], y[train], alpha = 0)
plot(cv.out)

bestlam <- cv.out$lambda.min
bestlam

ridge.pred <- predict(ridge.mod, s = bestlam,
                      newx = x[test, ])
mean((ridge.pred - y.test)^2)

out <- glmnet(x, y, alpha = 0)
predict(out, type = "coefficients", s = bestlam)[1:20, ]
```

### The Lasso

```{r}
lasso.mod <- glmnet(x[train, ], y[train], alpha = 1,
                    lambda = grid)
plot(lasso.mod)

cv.out <- cv.glmnet(x[train, ], y[train], alpha = 1, parallel=TRUE)
plot(cv.out)

bestlam <- cv.out$lambda.min

cat('Min Lambda: ', bestlam, '\n 1Sd Lambda: ', cv.out$lambda.1se)

df_coef <- round(as.matrix(coef(cv.out, s=bestlam)), 2)

# See all contributing variables
df_coef[df_coef[, 1] != 0, ]

lasso.pred <- predict(lasso.mod, s = bestlam, newx = x[test, ])
mean((lasso.pred - y.test)^2)

out <- glmnet(x, y, alpha = 1, lambda = grid)
lasso.coef <- predict(out, type = "coefficients", s = bestlam)[1:20, ]
lasso.coef
lasso.coef[lasso.coef != 0]
```

