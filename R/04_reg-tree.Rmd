---
title: "Dog-SEP"
subtitle: "Machine learning: regression trees"
pagetitle: "Dog-SEP: reg tree"
author: "Radoslaw Panczak"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document: 
    highlight: pygments
    keep_md: no
    number_sections: yes
    theme: united
    toc: yes
    toc_depth: 4
    self_contained: true
    toc_float: yes
editor_options: 
  chunk_output_type: console
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = "../docs") })
---

<!-- ------------------------------------------------------------ --> 

```{r r-setup, include=FALSE}
set.seed(12345)
options(scipen = 999)
options(max.print = "75")

library(pacman)
p_load(tidyverse, magrittr, DT,
       caret, rpart, rpart.plot, Cubist)

import::from("sjmisc", "frq")
```

```{r knit-setup, include=FALSE}
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file(),
                     width = 75)

knitr::opts_chunk$set(
  echo = TRUE,
  cache = FALSE,
  prompt = FALSE,
  tidy = FALSE,
  comment = NA,
  message = FALSE,
  warning = FALSE,
  fig.width = 8, fig.height = 6, dpi = 300,
  out.width = "800px", out.height = "600px"
)
```

```{r include=FALSE}
MAE <- function(actual, predicted) {
  mean(abs(actual - predicted))  
}
```

<!-- ------------------------------------------------------------ --> 

# Background 

Using info from chapter 6 of *Machine Learning with R* [book](https://www.packtpub.com/product/machine-learning-with-r-third-edition/9781788295864), `caret` package [manual](https://topepo.github.io/caret/index.html), and `Cubist` package [vignette](https://cran.r-project.org/web/packages/Cubist/vignettes/cubist.html).  

<!-- ------------------------------------------------------------ --> 

# CV set up

3-fold CV.

```{r}
fit_control <- trainControl(method = "cv",
                            number = 3)
```

<!-- 10-fold CV repeated.     -->

```{r eval=FALSE, include=FALSE}
fit_control <- trainControl(method = "repeatedcv",
                            number = 10,
                            repeats = 10)
```

# Analysis with `rpart`

## Data 

```{r include=FALSE}
training <- read_rds("data/training.Rds")
testing <- read_rds("data/testing.Rds")
```

## Training 

```{r}
set.seed(12345)
m_rpart <- rpart(outcome ~ ., data = training)
# m_rpart <- rpart(outcome ~ ., data = training, weights = training_weights)
```

```{r echo=FALSE}
m_rpart
# summary(m_rpart)
```

```{r echo=FALSE}
rpart.plot(m_rpart, digits = 3)
```

```{r echo=FALSE}
rpart.plot(m_rpart, digits = 3, fallen.leaves = TRUE, type = 3, extra = 101)
```

```{r eval=FALSE, include=FALSE}
rattle::fancyRpartPlot(m_rpart)
```

## Prediction

```{r}
p_rpart <- predict(m_rpart, testing[-1])
```

Correlation:  

```{r}
cor(p_rpart, testing$outcome)
```

MAE between predicted and actual values:  

```{r}
MAE(p_rpart, testing$outcome)
```

```{r eval=FALSE, include=FALSE}
# mean absolute error between actual values and mean value
# mean(training$outcome) 
MAE(mean(training$outcome), testing$outcome)
```

Distro of predicted (red) nd actual (green): 

```{r echo=FALSE}
# summary(p_rpart)
# summary(testing$outcome)

ggplot() +
  geom_density(data = as.data.frame(p_rpart), aes(x = p_rpart), col = "firebrick") +
  geom_density(data = testing, aes(x = outcome), col = "forestgreen") 
```

## Tuning

```{r}
modelLookup("rpart")
```

Using grid of parameters to explore:  

```{r}
grid_tu <- expand.grid(cp = seq(0, 1, 0.01))

nrow(grid_tu)
```

```{r}
p_load(doParallel)

cl <- makePSOCKcluster(parallel::detectCores())
registerDoParallel(cl)

set.seed(12345)
m_rpart_tu <- train(outcome ~ .,
                    method = "rpart",
                    # method = "rpart2", # uses max tree depth
                    data = training,
                    # weights = training_weights, 
                    metric = "RMSE",
                    # metric = "Rsquared",
                    tuneGrid = grid_tu,
                    trControl = fit_control)

stopCluster(cl)
p_unload(doParallel)
```

```{r include=FALSE}
write_rds(m_rpart_tu, "results/m_rpart_tu.Rds")
```

```{r echo=FALSE}
# summary(m_rpart_tu)
ggplot(m_rpart_tu)
```

```{r}
m_rpart_tu$finalModel
```

```{r echo=FALSE}
rpart.plot(m_rpart_tu$finalModel, digits = 3)
```

```{r echo=FALSE}
rpart.plot(m_rpart_tu$finalModel, digits = 3, fallen.leaves = TRUE, type = 3, extra = 101)
```

```{r eval=FALSE, include=FALSE}
rattle::fancyRpartPlot(m_rpart_tu$finalModel)
```

```{r}
p_rpart_tu <- predict(m_rpart_tu, testing[-1])
```

Correlation:  

```{r}
cor(p_rpart_tu, testing$outcome)
```

MAE between predicted and actual values:  

```{r}
MAE(p_rpart_tu, testing$outcome)
```

Distro of predicted (red) nd actual (green): 

```{r echo=FALSE}
# summary(p_rpart)
# summary(testing$outcome)

ggplot() +
  geom_density(data = as.data.frame(p_rpart_tu), aes(x = p_rpart_tu), col = "firebrick") +
  geom_density(data = testing, aes(x = outcome), col = "forestgreen") 
```

<!-- ------------------------------------------------------------ --> 

# Analysis with `Cubist`

## Training   

```{r}
set.seed(12345)
m_cubist <- cubist(x = training[-1], y = training$outcome)
```

```{r echo=FALSE}
# m_cubist
summary(m_cubist)
```

## Prediction

```{r}
p_cubist <- predict(m_cubist, testing[-1])
```

Correlation:  

```{r}
cor(p_cubist, testing$outcome)
```

MAE between predicted and actual values:  

```{r}
MAE(testing$outcome, p_cubist)
```

Distro of predicted (red) nd actual (green): 

```{r echo=FALSE}
ggplot() +
  geom_density(data = as.data.frame(p_cubist), aes(x = p_cubist), col = "firebrick") +
  geom_density(data = testing, aes(x = outcome), col = "forestgreen") 
```

Variable importance:  

```{r}
varImp(m_cubist) %>% 
  as_tibble(rownames = "breed")

# vip::vi(m_cubist)
```

## Tuning

```{r}
modelLookup("cubist")
```

Using grid of parameters to explore:  

```{r}
grid_tu <-  expand.grid(committees = (50:100), 
                        neighbors = (5:9))

nrow(grid_tu)
```

```{r}
p_load(doParallel)

cl <- makePSOCKcluster(parallel::detectCores())
registerDoParallel(cl)

set.seed(12345)
m_cubist_tu <- train(outcome ~ .,
                     method = "cubist",
                     data = training,
                     # weights = training_weights, 
                     metric = "RMSE",
                     # metric = "Rsquared",
                     tuneGrid = grid_tu,
                     trControl = fit_control)

stopCluster(cl)
p_unload(doParallel)
```

```{r include=FALSE}
write_rds(m_cubist_tu, "results/m_cubist_tu.Rds")
```

```{r echo=FALSE}
# summary(m_cubist_tu)
ggplot(m_cubist_tu)
```

```{r}
m_cubist_tu$finalModel
```

```{r}
p_cubist_tu <- predict(m_cubist_tu, testing[-1])
```

Correlation:  

```{r}
cor(p_cubist_tu, testing$outcome)
```

MAE between predicted and actual values:  

```{r}
MAE(p_cubist_tu, testing$outcome)
```

Distro of predicted (red) nd actual (green): 

```{r echo=FALSE}
# summary(p_rpart)
# summary(testing$outcome)

ggplot() +
  geom_density(data = as.data.frame(p_cubist_tu), aes(x = p_cubist_tu), col = "firebrick") +
  geom_density(data = testing, aes(x = outcome), col = "forestgreen") 
```

Variable importance:  

```{r}
varImp(m_cubist_tu)
```

Rules:  

```{r}
p_load(tidyrules)

tidyRules(m_cubist_tu$finalModel) %>% 
  datatable()
```